# Conversational AI with RAG ğŸ§ ğŸ’¬

## ğŸ“Œ Objective
This project demonstrates how to build a Conversational AI system that answers questions based on custom data sources (text, PDF, JSON, or images).

It integrates open-source and proprietary LLMs with LangChain and Vector Databases to provide accurate, context-aware responses.

---

## âš™ï¸ Tech Stack
- **Frameworks**: LangChain, Gradio  
- **Vector DB**: Chroma, FAISS 
- **LLMs**: OpenAI (GPT-4/4o-mini)
- **Tools**: Prompt Engineering, Chains  

---

## ğŸš€ Features
- Ingests multiple document types  
- Embedding + VectorDB retrieval  
- Works with multiple LLMs  
- Simple Gradio chat interface  

---

## ğŸ› ï¸ Setup & Installation

1ï¸âƒ£ Clone the repository

git clone https://github.com/your-username/conversational-ai-rag.git

cd conversational-ai-rag


2ï¸âƒ£ Create virtual environment

python -m venv venv

source venv/bin/activate   # On Mac/Linux

venv\Scripts\activate      # On Windows


3ï¸âƒ£ Install dependencies

pip install -r requirements.txt


4ï¸âƒ£ Run Colab

colab notebook

## ğŸ¯ Usage

Place your documents in the data/ folder.

Run the notebook Final_Gen-AI_Project_Jay.ipynb.

Choose your preferred LLM (OpenAI, HuggingFace, etc.).

Start chatting via the Gradio interface.

## ğŸ” RAG Workflow

Document Loader â†’ Load data (PDF, text, JSON, images)

Text Splitter â†’ Chunk data for embedding

Embedding Model â†’ Convert chunks into vector representations

VectorDB â†’ Store & retrieve embeddings

LLM Integration â†’ Answer questions using retrieval + prompt engineering

Gradio UI â†’ Chat with your data
