# -*- coding: utf-8 -*-
"""Final_Gen-AI_Project_Jay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soDvpFFmh0uquhm71yWYy30_MWSURbFo

## **Objective**
  * **We are going to create our conversational AI, that will answer the questions based on the given data source (pdf, text, img, json)**

* **`Open Source Model`: Deepseek, Mixtral, Zephyr, Dolly, Llama, Phi (HuggingFace, Unsloth, replicate)**

* **`Proprietry Models`: OpenAI, Google Gemini & PaLm, Microsoft**

### **RAG Application**
* **Indexing**
  * **Load the data: Document Loader**
  * **Split the data: Text Splitter**
  * **Embed the data: Embedding Model**
  * **Save the data into a DB: VectorDB (`Chroma` and PineCone)**
<hr>
* **Retrieval**
  * **Setup LLM: ChatGPT (4o-mini, GPT-4)**
  * **Prompt Engineering (To make sure the model works fine)**
  * **Connect & Chain these all together: Chain**
  * **Utilize the LLM: Test**
<hr>
  * **Interface for having results as output: Gradio**

# **Step 1 - Requirement Phase**

* **Data Source: `plain text file`**
* **Framework: `Langchain`**
"""

!pip install langchain langchain_community langchain_chroma

"""### **Importing the dependencies**"""

from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers.string import StrOutputParser

"""# **Step 2 - Document Processing**

### **1. Taking a plain text file**

**Link: https://drive.google.com/file/d/1z5FTeCvkrfHnMrSfbtlvHH1CpYKJ6udR/view?usp=sharing**
"""

with open('/content/2024_state_of_the_union.txt') as f:
  files = f.read()

print(files)



"""### **2. Split the data**

"""

text_split=CharacterTextSplitter(
    chunk_size=1000,    # no.of charcters in a chunk
    chunk_overlap=200,  # common part between the chunk i and i-1 {end 200 char of i-1 = start 200 char of i}
    length_function=len
)

"""### **3. Create the split / segment the documentation**"""

texts=text_split.create_documents([files])

"""### **Output**"""

len(texts)

texts[0]

"""# **Step 3 - Embed the data using Embedding Model**

### **Create the embeddings**
"""

from langchain.embeddings import HuggingFaceEmbeddings

embedding_model=HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

"""### **Databae Formation**"""

vectorDB = Chroma(
    collection_name='Jay',
    embedding_function=embedding_model
)

vectorDB

"""### **Load the documents in the DB**"""

storage_id = vectorDB.add_documents(texts)

len(storage_id)

storage_id[0]

"""1. Text

   └── Raw input text data (e.g., document, web page, transcript)

2. Split into Chunks

   └── Divide text into manageable chunks (e.g., by sentences or paragraphs)

3. Embedding Model

   └── Use a model (like OpenAI, Sentence-BERT) to convert text chunks into embeddings

4. Vectors

   └── Embeddings are high-dimensional numeric representations of the text

5. Vector Database

   └── Store these vectors in a database optimized for similarity search (e.g., FAISS, Pinecone, Weaviate)

6. Primary IDs

   └── Assign a unique identifier to each vector entry

7. Ensure Uniqueness

   └── Validate that each ID is distinct to avoid collisions or duplication

### **Similarity Searching using VecDB**
"""

res=vectorDB.similarity_search(
    query="What did the president say about Ketanji Brown Jackson",
    k=1
)

res

storage_id.index('e81e2fd9-1007-41bc-9b7c-3e93a5fcea81')

texts[36]

"""# **Step 4 - Setting up the Retrievals**

### **a. Create a retriever**
"""

retriever = vectorDB.as_retriever()

"""### **b. LLM Instance**"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-large')

if tokenizer.pad_token is None:
  tokenizer.add_special_tokens({'pad_toke':'[PAD]'})



model=AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-large')

model.resize_token_embeddings(len(tokenizer))

model.config.pad_token_id = tokenizer.pad_token_id

generator=pipeline(
    'text2text-generation',
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=150
)

from langchain.llms import HuggingFacePipeline

llm = HuggingFacePipeline(pipeline=generator)

"""#### **Other Examples**

* **HuggingFaceH4/zephyr-7b-beta**
* **Qwen/Qwen3-235B-A22B**

### **c. Design a Prompt**
"""

template= """Use the context provided to answer the question. If you don't know the answer, say you don't know.

Context:
{context}

Question: {question}
Answer:"""

custom_templete=PromptTemplate(
    template=template
)

custom_templete

"""**We have a template, model, database**

* **Can we connect them**
"""

rag_chain=(
    {'context':retriever,'question':RunnablePassthrough()}
    |custom_templete
    |llm
    |StrOutputParser()
)

# Input question
#      ↓
# Retriever → Format → Context
#      ↓
# {context, question}
#      ↓
# Prompt Template (custom_template)
#      ↓
# LLM (generate answer)
#      ↓
# StrOutputParser (final output string)

"""# **Step 5 - Test**"""

query = "What did the President say about Ukrain?"
answer = rag_chain.invoke(query)
answer

!pip install gradio

import gradio as gr

def chat(message,history):
  bot_message=rag_chain.invoke(message)
  history.append(message,bot_message)
  return history,history
with gr.Blocks() as demo:
  chatbot=gr.Chatbot()
  msg=gr.Textbox()
  clear=gr.Button('clear')
  msg.submit(chat,[msg,chatbot],[chatbot,chatbot])
  clear.click(lambda: None,None,chatbot,queue=False )
demo.launch()